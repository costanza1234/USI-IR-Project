{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sraping CHARITY NAVIGATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch data from Charity Navigator API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "charity_navigator_key = os.getenv('CHARITY_NAVIGATOR')\n",
    "\n",
    "CHARITY_NAVIGATOR_ENDPOINT = 'https://data.charitynavigator.org/'\n",
    "\n",
    "# Fetch data from Charity Navigator\n",
    "def fetch_charity_data(count):\n",
    "    QUERY = \"\"\"\n",
    "    query {\n",
    "        publicSearchFaceted(term: \"\", from: %d) {\n",
    "            size\n",
    "            from\n",
    "            term\n",
    "            result_count\n",
    "            results {\n",
    "                ein\n",
    "                name\n",
    "                mission\n",
    "                organization_url\n",
    "                charity_navigator_url\n",
    "                encompass_score\n",
    "                encompass_star_rating\n",
    "                encompass_publication_date\n",
    "                cause\n",
    "                street\n",
    "                street2\n",
    "                city\n",
    "                state\n",
    "                zip\n",
    "                country\n",
    "                highest_level_advisory\n",
    "                encompass_rating_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\" % count\n",
    "\n",
    "    headers = {\n",
    "        \"Stellate-Api-Token\": charity_navigator_key,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.post(\n",
    "        CHARITY_NAVIGATOR_ENDPOINT,\n",
    "        headers=headers,\n",
    "        json={\"query\": QUERY}\n",
    "    )\n",
    "    # Raise an error if the request fails\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Return the JSON response\n",
    "    return response.json()\n",
    "\n",
    "results = []\n",
    "\n",
    "\"\"\" for i in range(0, 10001, 10):\n",
    "   data = fetch_charity_data(i)\n",
    "   print(data)\n",
    "   results.append(data) \"\"\"\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('charity_navigator_data.json', 'w') as f:\n",
    "    f.write(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Helper function to validate URLs\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "# Fix and normalize URLs\n",
    "def fixURL(url):\n",
    "    if url is None:\n",
    "        return None\n",
    "    url = url.lower().strip()\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'https://' + url\n",
    "\n",
    "# Function to fetch the logo URL from the organization page\n",
    "def get_logo(url):\n",
    "    \"\"\"Fetch and return the logo URL from a given website URL.\"\"\"\n",
    "    print(f\"Fetching logo from {url}\")\n",
    "    if not is_valid_url(url):\n",
    "        print(f\"Invalid URL skipped: {url}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)  # Add timeout for safety\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for common logo patterns\n",
    "        logo = None\n",
    "        for tag in soup.find_all(['img', 'link']):\n",
    "            # Check for <img> tags with 'logo' in the class, id, or alt attribute\n",
    "            if tag.name == 'img' and any(keyword in (tag.get('class', []) + [tag.get('id', ''), tag.get('alt', '')]) for keyword in ['logo', 'brand']):\n",
    "                logo = tag\n",
    "                break\n",
    "            \n",
    "            # Check for <link> tags that might point to an icon/logo\n",
    "            if tag.name == 'link' and tag.get('rel') and 'icon' in tag.get('rel', []):\n",
    "                logo = tag\n",
    "                break\n",
    "\n",
    "        # Return the absolute URL of the logo\n",
    "        if logo and logo.get('src'):\n",
    "            return urljoin(url, logo['src'])\n",
    "        elif logo and logo.get('href'):\n",
    "            return urljoin(url, logo['href'])\n",
    "        \n",
    "        print(\"Logo not found on the page.\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching logo from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process the list of charities and fetch logos\n",
    "def get_logos(charities):\n",
    "    for charity in charities:\n",
    "        url = charity.get('organization_url')\n",
    "        if url:\n",
    "            charity['logoUrl'] = get_logo(url)\n",
    "        else:\n",
    "            charity['logoUrl'] = None\n",
    "    return charities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Charity Navigator data\n",
    "with open(\"charity_navigator_data.json\", \"r\") as f:\n",
    "    charity_navigator_data = json.load(f)\n",
    "\n",
    "# Prepare the charity data for processing\n",
    "charity_navigator_json = []\n",
    "for data in charity_navigator_data:\n",
    "    for charity in data['data']['publicSearchFaceted']['results']:\n",
    "        charity_navigator_json.append({\n",
    "            'name': charity['name'],\n",
    "            'mission': charity['mission'],\n",
    "            'cause': charity['cause'],\n",
    "            'city': charity['city'],\n",
    "            'country': charity['country'],\n",
    "            'organization_url': fixURL(charity.get('organization_url'))\n",
    "        })\n",
    "\n",
    "\n",
    "# Save the processed charity data to a new JSON file\n",
    "with open('charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 charities from charity_navigator.json and fetch their logos\n",
    "with open('charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "\n",
    "# Fetch logos for the first 10 charities\n",
    "get_logos(charity_navigator_json[:10])\n",
    "\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load charity_navigator.json\n",
    "with open('charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "    \n",
    "# For each element in charity_navigator_json, fetch the logo\n",
    "get_logos(charity_navigator_json)\n",
    "\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import json\n",
    "\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        # Get country alpha-2 code\n",
    "        country_code = pycountry.countries.lookup(country_name).alpha_2\n",
    "        # Map to continent code\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "        # Convert to continent name\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except LookupError:\n",
    "        return \"Unknown country\"\n",
    "\n",
    "# Open ./data/charity_navigator_logos.json \n",
    "# Read the JSON data from the file\n",
    "with open('scraping/data/charity_navigator_logos.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "    \n",
    "\n",
    "# For each organization, add a field 'continent' that maps to the continent of the country\n",
    "for organization in json_data:\n",
    "    organization[\"continent\"] = country_to_continent(organization[\"country\"])\n",
    "\n",
    "# Save the updated data to a JSON file\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(json_data, indent=2))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
