{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sraping CHARITY NAVIGATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Helper function to validate URLs\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "# Fix and normalize URLs\n",
    "def fixURL(url):\n",
    "    if url is None:\n",
    "        return None\n",
    "    url = url.lower()\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'https://' + url\n",
    "\n",
    "def get_logo(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        logo_candidates = []\n",
    "\n",
    "        # Keywords to detect logos\n",
    "        logo_keywords = ['logo', 'brand', 'header', 'main', 'icon']\n",
    "\n",
    "        # Keywords to exclude irrelevant images\n",
    "        exclude_keywords = ['menu', 'close', 'button', 'hamburger', 'arrow', 'banner', 'background']\n",
    "\n",
    "        def is_valid_logo(src, tag, width, height):\n",
    "            \"\"\"Determine if the image source is a valid logo.\"\"\"\n",
    "            if not src:\n",
    "                return False\n",
    "            src_lower = src.lower()\n",
    "\n",
    "            # Exclude based on keywords in the URL or filename\n",
    "            if any(exclude in src_lower for exclude in exclude_keywords):\n",
    "                return False\n",
    "\n",
    "            # Include based on keywords in src, alt, class, or id\n",
    "            alt = tag.get('alt', '').lower()\n",
    "            tag_class = ' '.join(tag.get('class', [])).lower()\n",
    "            tag_id = tag.get('id', '').lower()\n",
    "\n",
    "            if any(keyword in (src_lower + alt + tag_class + tag_id) for keyword in logo_keywords):\n",
    "                # Exclude overly large images that might be banners\n",
    "                if width and height:\n",
    "                    if int(width) > 500 or int(height) > 200:  # Too large, likely a banner\n",
    "                        return False\n",
    "                    if int(width) / int(height) > 5 or int(height) / int(width) > 5:  # Extreme aspect ratio\n",
    "                        return False\n",
    "                return True\n",
    "\n",
    "            return False\n",
    "\n",
    "        # Step 1: Meta tags (Open Graph and Twitter card)\n",
    "        meta_tags = [\n",
    "            {'property': 'og:image'}, {'name': 'og:image'},\n",
    "            {'property': 'twitter:image'}, {'name': 'twitter:image'}\n",
    "        ]\n",
    "        for tag in meta_tags:\n",
    "            meta = soup.find('meta', tag)\n",
    "            if meta and meta.get('content'):\n",
    "                logo_candidates.append((urljoin(url, meta['content']), 0, 0, 1))  # Priority: 1\n",
    "\n",
    "        # Step 2: <link> tags (exclude favicons and irrelevant icons)\n",
    "        for tag in soup.find_all('link', rel=True):\n",
    "            rel = tag.get('rel', [])\n",
    "            href = tag.get('href')\n",
    "            if href and 'icon' in rel and not re.search(r'favicon|menu|close|button|icon|banner', href, re.IGNORECASE):\n",
    "                logo_candidates.append((urljoin(url, href), 0, 0, 1))  # Priority: 1\n",
    "\n",
    "        # Step 3: <img> tags (prioritize based on class, alt, id)\n",
    "        for tag in soup.find_all('img'):\n",
    "            src = tag.get('src')\n",
    "            width = tag.get('width', '0').replace('px', '')\n",
    "            height = tag.get('height', '0').replace('px', '')\n",
    "\n",
    "            # Default size if width/height is not set\n",
    "            width = int(width) if width.isdigit() else 0\n",
    "            height = int(height) if height.isdigit() else 0\n",
    "\n",
    "            # Check if valid logo\n",
    "            if is_valid_logo(src, tag, width, height):\n",
    "                priority = 2 if 'logo' in str(tag.get('class', '')).lower() or 'icon' in str(tag.get('class', '')).lower() else 3\n",
    "                logo_candidates.append((urljoin(url, src), width, height, priority))\n",
    "\n",
    "        # Step 4: Sort candidates by priority, then size\n",
    "        if logo_candidates:\n",
    "            logo_candidates.sort(key=lambda x: (x[3], -(x[1] * x[2])))\n",
    "            return logo_candidates[0][0]\n",
    "\n",
    "        print(\"Logo not found on the page.\")\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching logo from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Process the list of charities and fetch logos\n",
    "def get_logos(charities):\n",
    "    idx = 0\n",
    "    for charity in charities:\n",
    "        print(f\"Processing {charity['name']}, index {idx}\")\n",
    "        url = charity.get('organization_url')\n",
    "        if url:\n",
    "            charity['logoUrl'] = get_logo(url)\n",
    "        else:\n",
    "            charity['logoUrl'] = None\n",
    "        idx += 1\n",
    "    return charities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch data from Charity Navigator API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "charity_navigator_key = os.getenv('CHARITY_NAVIGATOR')\n",
    "\n",
    "CHARITY_NAVIGATOR_ENDPOINT = 'https://data.charitynavigator.org/'\n",
    "\n",
    "# Fetch data from Charity Navigator\n",
    "def fetch_charity_data(count):\n",
    "    QUERY = \"\"\"\n",
    "    query {\n",
    "        publicSearchFaceted(term: \"\", from: %d) {\n",
    "            size\n",
    "            from\n",
    "            term\n",
    "            result_count\n",
    "            results {\n",
    "                ein\n",
    "                name\n",
    "                mission\n",
    "                organization_url\n",
    "                charity_navigator_url\n",
    "                encompass_score\n",
    "                encompass_star_rating\n",
    "                encompass_publication_date\n",
    "                cause\n",
    "                street\n",
    "                street2\n",
    "                city\n",
    "                state\n",
    "                zip\n",
    "                country\n",
    "                highest_level_advisory\n",
    "                encompass_rating_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\" % count\n",
    "\n",
    "    headers = {\n",
    "        \"Stellate-Api-Token\": charity_navigator_key,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.post(\n",
    "        CHARITY_NAVIGATOR_ENDPOINT,\n",
    "        headers=headers,\n",
    "        json={\"query\": QUERY}\n",
    "    )\n",
    "    # Raise an error if the request fails\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Return the JSON response\n",
    "    return response.json()\n",
    "\n",
    "results = []\n",
    "\n",
    "\"\"\" for i in range(0, 10001, 10):\n",
    "   data = fetch_charity_data(i)\n",
    "   print(data)\n",
    "   results.append(data) \"\"\"\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('charity_navigator_data.json', 'w') as f:\n",
    "    f.write(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Charity Navigator data\n",
    "with open(\"charity_navigator_data.json\", \"r\") as f:\n",
    "    charity_navigator_data = json.load(f)\n",
    "\n",
    "# Prepare the charity data for processing\n",
    "charity_navigator_json = []\n",
    "for data in charity_navigator_data:\n",
    "    for charity in data['data']['publicSearchFaceted']['results']:\n",
    "        charity_navigator_json.append({\n",
    "            'name': charity['name'],\n",
    "            'mission': charity['mission'],\n",
    "            'cause': charity['cause'],\n",
    "            'city': charity['city'],\n",
    "            'country': charity['country'],\n",
    "            'organization_url': fixURL(charity.get('organization_url'))\n",
    "        })\n",
    "\n",
    "\n",
    "# Save the processed charity data to a new JSON file\n",
    "with open('charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import json\n",
    "\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        # Get country alpha-2 code\n",
    "        country_code = pycountry.countries.lookup(country_name).alpha_2\n",
    "        # Map to continent code\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "        # Convert to continent name\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except LookupError:\n",
    "        return \"Unknown country\"\n",
    "\n",
    "# Open ./data/charity_navigator_logos.json \n",
    "# Read the JSON data from the file\n",
    "with open('scraping/data/charity_navigator_logos.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "    \n",
    "\n",
    "# For each organization, add a field 'continent' that maps to the continent of the country\n",
    "for organization in json_data:\n",
    "    organization[\"continent\"] = country_to_continent(organization[\"country\"])\n",
    "\n",
    "# Save the updated data to a JSON file\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(json_data, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mercy Ships International , index 0\n",
      "Processing International Rescue Committee, index 1\n",
      "Processing Doctors Without Borders, USA, index 2\n",
      "Processing International Relief Teams, index 3\n",
      "Processing World Central Kitchen Incorporated, index 4\n",
      "Processing UNICEF USA, index 5\n",
      "Logo not found on the page.\n",
      "Processing Global Refuge, index 6\n",
      "Processing National Audubon Society, index 7\n",
      "Processing National Council of YMCAs of the USA, index 8\n",
      "Processing MAP International, index 9\n"
     ]
    }
   ],
   "source": [
    "# Get the first 10 charities from charity_navigator.json and fetch their logos\n",
    "with open('scraping/data/charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "\n",
    "# Fetch logos for the first 10 charities\n",
    "get_logos(charity_navigator_json[:10])\n",
    "\n",
    "with open('scraping/data/charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load charity_navigator.json\n",
    "with open('charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "    \n",
    "# For each element in charity_navigator_json, fetch the logo\n",
    "get_logos(charity_navigator_json)\n",
    "\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
