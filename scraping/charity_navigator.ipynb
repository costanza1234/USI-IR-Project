{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sraping CHARITY NAVIGATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Helper function to validate URLs\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "# Fix and normalize URLs\n",
    "def fixURL(url):\n",
    "    if url is None:\n",
    "        return None\n",
    "    url = url.lower()\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'https://' + url\n",
    "\n",
    "\n",
    "\n",
    "def get_logo(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        logo_candidates = []\n",
    "\n",
    "        logo_keywords = ['logo']\n",
    "        exclude_keywords = ['menu', 'close', 'button', 'hamburger', 'arrow', 'background']\n",
    "\n",
    "        def is_valid_logo(src, tag, width, height):\n",
    "            if not src:\n",
    "                return False\n",
    "            src_lower = src.lower()\n",
    "\n",
    "            if any(exclude in src_lower for exclude in exclude_keywords):\n",
    "                return False\n",
    "\n",
    "            alt = tag.get('alt', '').lower()\n",
    "            tag_class = ' '.join(tag.get('class', [])).lower()\n",
    "            tag_id = tag.get('id', '').lower()\n",
    "\n",
    "            if any(keyword in (src_lower + alt + tag_class + tag_id) for keyword in logo_keywords):\n",
    "                if width and height and (width > 500 or height > 200):\n",
    "                    return False\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        def get_priority(src, tag):\n",
    "            src_lower = src.lower() if src else ''\n",
    "            tag_class = ' '.join(tag.get('class', [])).lower()\n",
    "            tag_id = tag.get('id', '').lower()\n",
    "            alt = tag.get('alt', '').lower()\n",
    "\n",
    "            if 'logo' in (src_lower + tag_class + tag_id + alt):\n",
    "                return 0\n",
    "            return 2\n",
    "\n",
    "        # Step 1: Recursive search in containers with \"logo\" keyword\n",
    "        for container in soup.find_all(True, class_=True):\n",
    "            tag_classes = ' '.join(container.get('class', [])).lower()\n",
    "            if 'logo' in tag_classes:\n",
    "                # Check for <svg> directly\n",
    "                svg = container.find('svg')\n",
    "                if svg:\n",
    "                    logo_candidates.append((container.prettify(), 0, 0, 0))\n",
    "                # Check for child <img>\n",
    "                img = container.find('img')\n",
    "                if img and img.get('src'):\n",
    "                    src = img.get('src')\n",
    "                    width = img.get('width', '0').replace('px', '')\n",
    "                    height = img.get('height', '0').replace('px', '')\n",
    "                    width = int(width) if width.isdigit() else 0\n",
    "                    height = int(height) if height.isdigit() else 0\n",
    "\n",
    "                    if is_valid_logo(src, img, width, height):\n",
    "                        priority = get_priority(src, img)\n",
    "                        logo_candidates.append((urljoin(url, src), width, height, priority))\n",
    "\n",
    "        # Step 2: Meta tags (Open Graph/Twitter)\n",
    "        meta_tags = [{'property': 'og:image'}, {'name': 'og:image'}, {'property': 'twitter:image'}]\n",
    "        for tag in meta_tags:\n",
    "            meta = soup.find('meta', tag)\n",
    "            if meta and meta.get('content'):\n",
    "                logo_candidates.append((urljoin(url, meta['content']), 0, 0, 1))\n",
    "\n",
    "        # Step 3: <link> tags for icons\n",
    "        for tag in soup.find_all('link', rel=True):\n",
    "            rel = tag.get('rel', [])\n",
    "            href = tag.get('href')\n",
    "            if href and 'icon' in rel and 'favicon' not in href.lower():\n",
    "                logo_candidates.append((urljoin(url, href), 0, 0, 2))\n",
    "\n",
    "        # Step 4: General <img> tags\n",
    "        for tag in soup.find_all('img'):\n",
    "            src = tag.get('src')\n",
    "            width = tag.get('width', '0').replace('px', '')\n",
    "            height = tag.get('height', '0').replace('px', '')\n",
    "            width = int(width) if width.isdigit() else 0\n",
    "            height = int(height) if height.isdigit() else 0\n",
    "\n",
    "            if is_valid_logo(src, tag, width, height):\n",
    "                priority = get_priority(src, tag)\n",
    "                logo_candidates.append((urljoin(url, src), width, height, priority))\n",
    "\n",
    "        # Step 5: Sort candidates and select the best\n",
    "        if logo_candidates:\n",
    "            logo_candidates.sort(key=lambda x: (x[3], -(x[1] * x[2])))\n",
    "            best_candidate = logo_candidates[0][0]\n",
    "\n",
    "            if \"<svg\" in best_candidate:\n",
    "                print(\"SVG logo detected.\")\n",
    "                return best_candidate\n",
    "            return best_candidate\n",
    "\n",
    "        print(\"Logo not found on the page.\")\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching logo from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process the list of charities and fetch logos\n",
    "def get_logos(charities):\n",
    "    idx = 0\n",
    "    for charity in charities:\n",
    "        print(f\"Processing {charity['name']}, index {idx}\")\n",
    "        url = charity.get('organization_url')\n",
    "        if url:\n",
    "            charity['logoUrl'] = get_logo(url)\n",
    "        else:\n",
    "            charity['logoUrl'] = None\n",
    "        idx += 1\n",
    "    return charities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch data from Charity Navigator API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "charity_navigator_key = os.getenv('CHARITY_NAVIGATOR')\n",
    "\n",
    "CHARITY_NAVIGATOR_ENDPOINT = 'https://data.charitynavigator.org/'\n",
    "\n",
    "# Fetch data from Charity Navigator\n",
    "def fetch_charity_data(count):\n",
    "    QUERY = \"\"\"\n",
    "    query {\n",
    "        publicSearchFaceted(term: \"\", from: %d) {\n",
    "            size\n",
    "            from\n",
    "            term\n",
    "            result_count\n",
    "            results {\n",
    "                ein\n",
    "                name\n",
    "                mission\n",
    "                organization_url\n",
    "                charity_navigator_url\n",
    "                encompass_score\n",
    "                encompass_star_rating\n",
    "                encompass_publication_date\n",
    "                cause\n",
    "                street\n",
    "                street2\n",
    "                city\n",
    "                state\n",
    "                zip\n",
    "                country\n",
    "                highest_level_advisory\n",
    "                encompass_rating_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\" % count\n",
    "\n",
    "    headers = {\n",
    "        \"Stellate-Api-Token\": charity_navigator_key,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.post(\n",
    "        CHARITY_NAVIGATOR_ENDPOINT,\n",
    "        headers=headers,\n",
    "        json={\"query\": QUERY}\n",
    "    )\n",
    "    # Raise an error if the request fails\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Return the JSON response\n",
    "    return response.json()\n",
    "\n",
    "results = []\n",
    "\n",
    "\"\"\" for i in range(0, 10001, 10):\n",
    "   data = fetch_charity_data(i)\n",
    "   print(data)\n",
    "   results.append(data) \"\"\"\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('charity_navigator_data.json', 'w') as f:\n",
    "    f.write(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Charity Navigator data\n",
    "with open(\"charity_navigator_data.json\", \"r\") as f:\n",
    "    charity_navigator_data = json.load(f)\n",
    "\n",
    "# Prepare the charity data for processing\n",
    "charity_navigator_json = []\n",
    "for data in charity_navigator_data:\n",
    "    for charity in data['data']['publicSearchFaceted']['results']:\n",
    "        charity_navigator_json.append({\n",
    "            'name': charity['name'],\n",
    "            'mission': charity['mission'],\n",
    "            'cause': charity['cause'],\n",
    "            'city': charity['city'],\n",
    "            'country': charity['country'],\n",
    "            'organization_url': fixURL(charity.get('organization_url'))\n",
    "        })\n",
    "\n",
    "\n",
    "# Save the processed charity data to a new JSON file\n",
    "with open('charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import json\n",
    "\n",
    "def country_to_continent(country_name):\n",
    "    try:\n",
    "        # Get country alpha-2 code\n",
    "        country_code = pycountry.countries.lookup(country_name).alpha_2\n",
    "        # Map to continent code\n",
    "        continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "        # Convert to continent name\n",
    "        return pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    except LookupError:\n",
    "        return \"Unknown country\"\n",
    "\n",
    "# Open ./data/charity_navigator_logos.json \n",
    "# Read the JSON data from the file\n",
    "with open('scraping/data/charity_navigator_logos.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "    \n",
    "\n",
    "# For each organization, add a field 'continent' that maps to the continent of the country\n",
    "for organization in json_data:\n",
    "    organization[\"continent\"] = country_to_continent(organization[\"country\"])\n",
    "\n",
    "# Save the updated data to a JSON file\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(json_data, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mercy Ships International , index 0\n",
      "Processing International Rescue Committee, index 1\n",
      "Processing Doctors Without Borders, USA, index 2\n",
      "Processing International Relief Teams, index 3\n",
      "Processing World Central Kitchen Incorporated, index 4\n",
      "Processing UNICEF USA, index 5\n",
      "SVG logo detected.\n",
      "Processing Global Refuge, index 6\n",
      "SVG logo detected.\n",
      "Processing National Audubon Society, index 7\n",
      "Processing National Council of YMCAs of the USA, index 8\n",
      "SVG logo detected.\n",
      "Processing MAP International, index 9\n",
      "Processing Amnesty International USA, index 10\n",
      "Error fetching logo from https://www.amnestyusa.org: 403 Client Error: Forbidden for url: https://www.amnestyusa.org/\n",
      "Processing Conservation International, index 11\n",
      "Processing World Resources Institute, index 12\n",
      "Processing National Center for Missing & Exploited Children, index 13\n",
      "Processing Goodwill Industries International Inc., index 14\n",
      "Processing Heart to Heart International, index 15\n",
      "Error fetching logo from https://www.hearttoheart.org: 403 Client Error: Forbidden for url: https://www.hearttoheart.org/\n",
      "Processing World Vision, index 16\n",
      "Processing Action Against Hunger USA, index 17\n",
      "SVG logo detected.\n",
      "Processing Operation Blessing International, index 18\n",
      "Processing GlobalGiving, index 19\n"
     ]
    }
   ],
   "source": [
    "# Get the first 10 charities from charity_navigator.json and fetch their logos\n",
    "with open('scraping/data/charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "\n",
    "# Fetch logos for the first 10 charities\n",
    "get_logos(charity_navigator_json[:20])\n",
    "\n",
    "with open('scraping/data/charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load charity_navigator.json\n",
    "with open('charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "    \n",
    "# For each element in charity_navigator_json, fetch the logo\n",
    "get_logos(charity_navigator_json)\n",
    "\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
