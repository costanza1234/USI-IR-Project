{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do\n",
    "Scrape the websites in order to extract the following information:\n",
    "- Name \n",
    "- Logo\n",
    "- Location \n",
    "- Themes (i.e. children, homeless, medicine...)\n",
    "- Description / mission\n",
    "- URL of their website\n",
    "- Year of foundation (This information is sometimes implicit: can be computed based on Number of year of activity)\n",
    "\n",
    "### What to use\n",
    "- Requests\n",
    "- BeautifulSoup\n",
    "- Scrapy\n",
    "\n",
    "### Websites to scrape\n",
    "urls = [\n",
    "    \"https://www.charitynavigator.org\",\n",
    "    \"https://www.globalgiving.org\",\n",
    "    \"https://www.guidestar.org\",\n",
    "]\n",
    "\n",
    "### Charity Navigator API\n",
    "https://charity-navigator.stellate.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch data from Charity Navigator API\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "charity_navigator_key = os.getenv('CHARITY_NAVIGATOR')\n",
    "\n",
    "CHARITY_NAVIGATOR_ENDPOINT = 'https://data.charitynavigator.org/'\n",
    "\n",
    "# Fetch data from Charity Navigator\n",
    "def fetch_charity_data(count):\n",
    "    QUERY = \"\"\"\n",
    "    query {\n",
    "        publicSearchFaceted(term: \"\", from: %d) {\n",
    "            size\n",
    "            from\n",
    "            term\n",
    "            result_count\n",
    "            results {\n",
    "                ein\n",
    "                name\n",
    "                mission\n",
    "                organization_url\n",
    "                charity_navigator_url\n",
    "                encompass_score\n",
    "                encompass_star_rating\n",
    "                encompass_publication_date\n",
    "                cause\n",
    "                street\n",
    "                street2\n",
    "                city\n",
    "                state\n",
    "                zip\n",
    "                country\n",
    "                highest_level_advisory\n",
    "                encompass_rating_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"\"\" % count\n",
    "\n",
    "    headers = {\n",
    "        \"Stellate-Api-Token\": charity_navigator_key,\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.post(\n",
    "        CHARITY_NAVIGATOR_ENDPOINT,\n",
    "        headers=headers,\n",
    "        json={\"query\": QUERY}\n",
    "    )\n",
    "    # Raise an error if the request fails\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Return the JSON response\n",
    "    return response.json()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(0, 10001, 10):\n",
    "   data = fetch_charity_data(i)\n",
    "   print(data)\n",
    "   results.append(data)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('charity_navigator_data.json', 'w') as f:\n",
    "    f.write(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Helper function to validate URLs\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "# Fix and normalize URLs\n",
    "def fixURL(url):\n",
    "    if url is None:\n",
    "        return None\n",
    "    url = url.lower().strip()\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'https://' + url\n",
    "\n",
    "# ************************************** CHARITY NAVIGATOR CHARITIES ************************************** #\n",
    "\n",
    "# Load Charity Navigator data\n",
    "with open(\"charity_navigator_data.json\", \"r\") as f:\n",
    "    charity_navigator_data = json.load(f)\n",
    "\n",
    "# Prepare the charity data for processing\n",
    "charity_navigator_json = []\n",
    "for data in charity_navigator_data:\n",
    "    for charity in data['data']['publicSearchFaceted']['results']:\n",
    "        charity_navigator_json.append({\n",
    "            'name': charity['name'],\n",
    "            'mission': charity['mission'],\n",
    "            'cause': charity['cause'],\n",
    "            'city': charity['city'],\n",
    "            'country': charity['country'],\n",
    "            'organization_url': fixURL(charity.get('organization_url'))\n",
    "        })\n",
    "\n",
    "\n",
    "# Save the processed charity data to a new JSON file\n",
    "with open('charity_navigator.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the logo URL from the organization page\n",
    "def get_logo(url):\n",
    "    print(f\"Fetching logo from {url}\")\n",
    "    if not is_valid_url(url):\n",
    "        print(f\"Invalid URL skipped: {url}\")\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)  # Add timeout for safety\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        logo = soup.find('img', class_='logo')\n",
    "        return logo['src'] if logo else None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching logo from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process the list of charities and fetch logos\n",
    "def get_logos(charities):\n",
    "    for charity in charities:\n",
    "        url = charity.get('organization_url')\n",
    "        if url:\n",
    "            charity['logoUrl'] = get_logo(url)\n",
    "        else:\n",
    "            charity['logoUrl'] = None\n",
    "    return charities\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" # load charity_navigator.json\n",
    "with open('charity_navigator.json', 'r') as f:\n",
    "    charity_navigator_json = json.load(f)\n",
    "    \n",
    "# For each element in charity_navigator_json, fetch the logo\n",
    "get_logos(charity_navigator_json)\n",
    "\n",
    "with open('charity_navigator_logos.json', 'w') as f:\n",
    "    f.write(json.dumps(charity_navigator_json, indent=2)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************** GLOBAL GIVING CHARITIES ************************************** #\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# Read the XML data from the file\n",
    "with open('organizations.xml', 'r') as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "# Parse the XML data\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "# Extract data from all organization elements\n",
    "global_giving_json = []\n",
    "for organization in root.findall(\"organization\"):\n",
    "    org_data = {\n",
    "        \"name\": organization.find(\"name\").text,\n",
    "        \"city\": organization.find(\"city\").text,\n",
    "        \"country\": organization.find(\"country\").text,\n",
    "        \"activeProjects\": organization.find(\"activeProjects\").text,\n",
    "        \"totalProjects\": organization.find(\"totalProjects\").text,\n",
    "        \"mission\": organization.find(\"mission\").text.strip() if organization.find(\"mission\") is not None and organization.find(\"mission\").text is not None else \"\",\n",
    "        \"organization_url\": organization.find(\"url\").text,\n",
    "        \"logoUrl\": organization.find(\"logoUrl\").text if organization.find(\"logoUrl\") is not None else None,\n",
    "        \"cause\": [\n",
    "            {\n",
    "                \"name\": theme.find(\"name\").text\n",
    "            }\n",
    "            for theme in organization.find(\"themes\").findall(\"theme\")\n",
    "        ],\n",
    "        \"countries_of_operation\": [\n",
    "            {\n",
    "                \"name\": country.find(\"name\").text\n",
    "            }\n",
    "            for country in organization.find(\"countries\").findall(\"country\")\n",
    "        ],\n",
    "    }\n",
    "    global_giving_json.append(org_data)\n",
    "\n",
    "# Save the extracted data to a JSON file\n",
    "with open('global_giving.json', 'w') as f:\n",
    "    f.write(json.dumps(global_giving_json, indent=2))\n",
    "    \n",
    "# Display the extracted data\n",
    "#pprint.pprint(organizations_data[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
